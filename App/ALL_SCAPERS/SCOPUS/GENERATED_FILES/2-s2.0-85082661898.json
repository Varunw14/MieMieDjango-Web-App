{"Title": "It's New, but Is It Good? How Generalization and Uncertainty Guide the Exploration of Novel Options", "Year": 2020, "Source": "J. Exp. Psychol. Gen.", "Volume": null, "Issue": null, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 4, "DOI": "10.1037/xge0000749", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85082661898&origin=inward", "Abstract": "\u00a9 2020 American Psychological Association.How do people decide whether to try out novel options as opposed to tried-and-tested ones? We argue that they infer a novel option's reward from contextual information learned from functional relations and take uncertainty into account when making a decision. We propose a Bayesian optimization model to describe their learning and decision making. This model relies on similarity-based learning of functional relationships between features and rewards, and a choice rule that balances exploration and exploitation by combining predicted rewards and the uncertainty of these predictions. Our model makes 2 main predictions. First, decision makers who learn functional relationships will generalize based on the learned reward function, choosing novel options only if their predicted reward is high. Second, they will take uncertainty about the function into account, and prefer novel options that can reduce this uncertainty. We test these predictions in 3 preregistered experiments in which we examine participants' preferences for novel options using a feature-based multiarmed bandit task in which rewards are a noisy function of observable features. Our results reveal strong evidence for functional exploration and moderate evidence for uncertainty-guided exploration. However, whether or not participants chose a novel option also depended on their attention, as well as reflecting on the value of the options. These results advance our understanding of people's reactions in the face of novelty.", "AuthorKeywords": ["Decision making", "Exploration-exploitation", "Function learning", "Novelty", "Reinforcement learning"], "IndexKeywords": null, "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85082661898", "SubjectAreas": [["Experimental and Cognitive Psychology", "PSYC", "3205"], ["Psychology (all)", "PSYC", "3200"], ["Developmental Neuroscience", "NEUR", "2806"]], "AuthorData": {"54797384600": {"Name": "Stojic H.", "AuthorID": "54797384600", "AffiliationID": null, "AffiliationName": null}, "7202482374": {"Name": "Schulz E.", "AuthorID": "7202482374", "AffiliationID": null, "AffiliationName": null}, "56001038500": {"Name": "Analytis P.P.", "AuthorID": "56001038500", "AffiliationID": null, "AffiliationName": null}, "23470488800": {"Name": "Speekenbrink M.", "AuthorID": "23470488800", "AffiliationID": null, "AffiliationName": null}}}