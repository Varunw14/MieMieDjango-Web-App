{"Title": "Slim densepose: Thrifty learning from sparse annotations and motion cues", "Year": 2019, "Source": "Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit", "Volume": "2019-June", "Issue": null, "Art.No": null, "PageStart": 10907, "PageEnd": 10915, "CitedBy": 7, "DOI": "10.1109/CVPR.2019.01117", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85078794638&origin=inward", "Abstract": "\u00a9 2019 IEEE.DensePose supersedes traditional landmark detectors by densely mapping image pixels to body surface coordinates. This power, however, comes at a greatly increased annotation cost, as supervising the model requires to manually label hundreds of points per pose instance. In this work, we thus seek methods to significantly slim down the DensePose annotations, proposing more efficient data collection strategies. In particular, we demonstrate that if annotations are collected in video frames, their efficacy can be multiplied for free by using motion cues. To explore this idea, we introduce DensePose-Track, a dataset of videos where selected frames are annotated in the traditional DensePose manner. Then, building on geometric properties of the DensePose mapping, we use the video dynamic to propagate ground-truth annotations in time as well as to learn from Siamese equivariance constraints. Having performed exhaustive empirical evaluation of various data annotation and learning strategies, we demonstrate that doing so can deliver significantly improved pose estimation results over strong baselines. However, despite what is suggested by some recent works, we show that merely synthesizing motion patterns by applying geometric transformations to isolated frames is significantly less effective, and that motion cues help much more when they are extracted from videos.", "AuthorKeywords": ["And Body Pose", "Datasets and Evaluation", "Deep Learning", "Face", "Gesture", "Motion and Tracking"], "IndexKeywords": ["Body pose", "Datasets and Evaluation", "Face", "Gesture", "Motion and tracking"], "DocumentType": "Conference Proceeding", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85078794638", "SubjectAreas": [["Software", "COMP", "1712"], ["Computer Vision and Pattern Recognition", "COMP", "1707"]], "AuthorData": {"55421132600": {"Name": "Neverova N.", "AuthorID": "55421132600", "AffiliationID": "60111190", "AffiliationName": "Facebook AI Research"}, "14036614600": {"Name": "Vedaldi A.", "AuthorID": "14036614600", "AffiliationID": "60111190", "AffiliationName": "Facebook AI Research"}, "57200619105": {"Name": "Thewlis J.", "AuthorID": "57200619105", "AffiliationID": "60026851", "AffiliationName": "University of Oxford"}, "56458738500": {"Name": "Guler R.A.", "AuthorID": "56458738500", "AffiliationID": null, "AffiliationName": "Ariel AI"}, "9250105400": {"Name": "Kokkinos I.", "AuthorID": "9250105400", "AffiliationID": null, "AffiliationName": "Ariel AI"}}}