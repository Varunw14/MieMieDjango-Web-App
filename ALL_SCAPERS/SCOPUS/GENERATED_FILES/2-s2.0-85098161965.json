{"Title": "The interrelationship between the face and vocal tract configuration during audiovisual speech", "Year": 2020, "Source": "Proc. Natl. Acad. Sci. U. S. A.", "Volume": "117", "Issue": 51, "Art.No": null, "PageStart": 32791, "PageEnd": 32798, "CitedBy": 0, "DOI": "10.1073/pnas.2006192117", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85098161965&origin=inward", "Abstract": "\u00a9 2020 National Academy of Sciences. All rights reserved.It is well established that speech perception is improved when we are able to see the speaker talking along with hearing their voice, especially when the speech is noisy. While we have a good understanding of where speech integration occurs in the brain, it is unclear how visual and auditory cues are combined to improve speech perception. One suggestion is that integration can occur as both visual and auditory cues arise from a common generator: the vocal tract. Here, we investigate whether facial and vocal tract movements are linked during speech production by comparing videos of the face and fast magnetic resonance (MR) image sequences of the vocal tract. The joint variation in the face and vocal tract was extracted using an application of principal components analysis (PCA), and we demonstrate that MR image sequences can be reconstructed with high fidelity using only the facial video and PCA. Reconstruction fidelity was significantly higher when images from the two sequences corresponded in time, and including implicit temporal information by combining contiguous frames also led to a significant increase in fidelity. A \"Bubbles\" technique was used to identify which areas of the face were important for recovering information about the vocal tract, and vice versa, on a frame-by-frame basis. Our data reveal that there is sufficient information in the face to recover vocal tract shape during speech. In addition, the facial and vocal tract regions that are important for reconstruction are those that are used to generate the acoustic speech signal.", "AuthorKeywords": ["Audiovisual", "PCA", "Speech"], "IndexKeywords": null, "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-85098161965", "SubjectAreas": [["Multidisciplinary", "MULT", "1000"]], "AuthorData": {"56921689900": {"Name": "Scholes C.", "AuthorID": "56921689900", "AffiliationID": "60015138", "AffiliationName": "Visual Neuroscience Group, School of Psychology, University of Nottingham"}, "57217503729": {"Name": "Johnston A.", "AuthorID": "57217503729", "AffiliationID": "60015138", "AffiliationName": "Visual Neuroscience Group, School of Psychology, University of Nottingham"}, "8266532900": {"Name": "Skipper J.I.", "AuthorID": "8266532900", "AffiliationID": "60022148", "AffiliationName": "Experimental Psychology, University College London"}}}